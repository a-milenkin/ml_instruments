{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <h1> üß∂ –°–≤—è–∑–∫–∞ `Lightning` + `ClearML` –≤ –∑–∞–¥–∞—á–∞—Ö CV. üñº</h1> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ –Ω–æ—É—Ç–±—É–∫–∞\n",
    "<img src='../images/CV.webp' align=\"right\" width=\"508\" height=\"428\" >\n",
    "<br>\n",
    "\n",
    "<p><font size=\"3\" face=\"Arial\" font-size=\"large\"><ul type=\"square\">\n",
    "    \n",
    "<li><a href=\"#p1\">üßê –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–≤—è–∑–∫—É –≤ –¥–µ–ª–µ!</a></li>\n",
    "<li><a href=\"#p2\">üé® –ó–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π</a></li>\n",
    "<li><a href=\"#p7\">üéõ –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.</a></li>\n",
    "<li><a href=\"#p3\">üß∂ –°–≤—è–∑–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ üß©</a></li>\n",
    "<li><a href=\"#p6\">üß∏ –í—ã–≤–æ–¥—ã –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏—è ‚úÖ </a></li>\n",
    "\n",
    "\n",
    "    \n",
    "</ul></font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüéì –í —ç—Ç–æ–º —É—Ä–æ–∫–µ —Ä–∞–∑–±–µ—Ä–µ–º –∫–∞–∫ —Å–≤—è–∑–∫–∞ **PyTorch Lightning** c **ClearML** –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ CV-–∑–∞–¥–∞—á–∞—Ö. ü™Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "–°–≤—è–∑–∫–∞ **ClearML** —Å **PyTorch Lightning** —Ö–æ—Ä–æ—à–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–µ–±—è –≤ –∑–∞–¥–∞—á–∞—Ö –∏–∑ CV-–¥–æ–º–µ–Ω–∞.\n",
    "\n",
    "* –£–¥–æ–±–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å–µ—Ç–∫–∏, –æ—Å–æ–±–µ–Ω–Ω–æ, –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Å–µ–≤—Ä–≤–µ—Ä–∞—Ö, –∞ –Ω–µ –≤ –Ω–æ—É—Ç–±—É–∫–∞—Ö.\n",
    "* –õ–µ–≥–∫–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ñ–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –±—ç–∫–±–æ–Ω–∞\n",
    "* –£–¥–æ–±–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (GAN –∏.—Ç.–ø)\n",
    "* –ü–ª—é—Å –≤—Å–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –≤ –ø—Ä–æ—à–ª–æ–º —É—Ä–æ–∫–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center id=\"p1\">  üßê –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–≤—è–∑–∫—É –≤ –¥–µ–ª–µ!</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install clearml tensorboard lightning torchvision torchmetrics -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from lightning import LightningModule, Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from clearml import Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "–í–≤–µ–¥–∏—Ç–µ API Access —Ç–æ–∫–µ–Ω:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "–í–≤–µ–¥–∏—Ç–µ API Secret —Ç–æ–∫–µ–Ω:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "#from getpass import getpass\n",
    "# –í–≤–µ–¥–∏—Ç–µ –ø–æ–æ—á–µ—Ä—ë–¥–Ω–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏ –≤ –ø–æ—è–≤–∏–≤—à–µ–º—Å—è –æ–∫–Ω–µ (–∫–æ–¥ –∏–∑–º–µ–Ω—è—Ç—å –Ω–µ –Ω—É–∂–Ω–æ)\n",
    "#access_key = getpass(prompt=\"–í–≤–µ–¥–∏—Ç–µ API Access —Ç–æ–∫–µ–Ω: \")\n",
    "#secret_key = getpass(prompt=\"–í–≤–µ–¥–∏—Ç–µ API Secret —Ç–æ–∫–µ–Ω: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# #  –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–æ–∏ api-–∫–ª—é—á–∏\n",
    "# %env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "# %env CLEARML_API_HOST=https://api.clear.ml\n",
    "# %env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "\n",
    "# %env CLEARML_API_ACCESS_KEY=$access_key\n",
    "# %env CLEARML_API_SECRET_KEY=$secret_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> üé® –ó–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π </center>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "–ù–∞–ø–∏—à–µ–º –∏ –æ–±—É—á–∏–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Å–µ—Ç–∫—É –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ `CIFAR10`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=bc5186058e6e4411aa63848e1a1bb9f5\n",
      "2025-02-17 16:16:15,713 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/0cb5f71423834f7ba18c3a50e5989fd3/experiments/bc5186058e6e4411aa63848e1a1bb9f5/output/log\n"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –≤ ClearML\n",
    "task = Task.init(\n",
    "    project_name=\"Image Classification\",\n",
    "    task_name=\"CIFAR10 Training with Image Logging\",\n",
    "    task_type=Task.TaskTypes.training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'num_epochs': 3, 'batch_size': 24, 'learning_rate': 0.001, 'dropout_rate': 0.25, 'num_images_to_log': 5}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 24,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"dropout_rate\": 0.25,\n",
    "    \"num_images_to_log\": 5,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "}\n",
    "config = task.connect(config)  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ ClearML\n",
    "print(\"Hyperparameters:\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# –ö–ª–∞—Å—Å—ã CIFAR-10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Model(LightningModule):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.25, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 6 * 6, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(84, num_classes),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        task.get_logger().report_scalar(\"Loss\", \"train\", loss.item(), self.global_step)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        acc = self.accuracy(outputs, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True)\n",
    "        task.get_logger().report_scalar(\"Loss\", \"val\", loss.item(), self.global_step)\n",
    "        task.get_logger().report_scalar(\"Accuracy\", \"val\", acc.item(), self.global_step)\n",
    "\n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "        if batch_idx == 0:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞\n",
    "            _, predicted = outputs.max(1)\n",
    "            for i in range(min(len(inputs), config[\"num_images_to_log\"])):\n",
    "                img = transforms.ToPILImage()(inputs[i].cpu())\n",
    "                pred_label = predicted[i].item()\n",
    "                true_label = labels[i].item()\n",
    "                title = f\"True: {classes[true_label]}, Pred: {classes[pred_label]}\"\n",
    "                task.get_logger().report_image(\n",
    "                    title=title,\n",
    "                    series=f\"Batch {self.current_epoch}\",\n",
    "                    iteration=self.global_step,\n",
    "                    image=img,\n",
    "                )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=self.hparams.learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=config[\"num_epochs\"],\n",
    "    accelerator=\"gpu\",# if Trainer.auto_device_count() > 0 else \"cpu\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name     | Type               | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model    | Sequential         | 81.3 K | train\n",
      "1 | loss_fn  | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------\n",
      "81.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "81.3 K    Total params\n",
      "0.325     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55b385d37f045499955eea3335ffb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "model = CIFAR10Model(\n",
    "    dropout_rate=config[\"dropout_rate\"],\n",
    "    learning_rate=config[\"learning_rate\"]\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID number is: bc5186058e6e4411aa63848e1a1bb9f5\n"
     ]
    }
   ],
   "source": [
    "print('Task ID number is: {}'.format(task.id))\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> üéõ –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=bc209c2c8a22429588be10ad2e145a63\n",
      "ClearML results page: https://app.clear.ml/projects/0cb5f71423834f7ba18c3a50e5989fd3/experiments/bc209c2c8a22429588be10ad2e145a63/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ClearML Task\n",
    "task = Task.init(\n",
    "    project_name=\"Image Classification\",\n",
    "    task_name=\"Fine-tuning ResNet18 on CIFAR10\",\n",
    "    task_type=Task.TaskTypes.training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'num_epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'fine_tune_start_epoch': 5, 'num_classes': 10, 'num_images_to_log': 5}\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ ClearML)\n",
    "config = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"fine_tune_start_epoch\": 5,  # –° –∫–∞–∫–æ–π —ç–ø–æ—Ö–∏ —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—Ç—å –±—ç–∫–±–æ–Ω\n",
    "    \"num_classes\": 10,\n",
    "    \"num_images_to_log\": 5,  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤ ClearML\n",
    "}\n",
    "config = task.connect(config)\n",
    "print(\"Hyperparameters:\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Lightning-–º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ResNet18\n",
    "class FineTuneResNet(LightningModule):\n",
    "    def __init__(self, num_classes=10, learning_rate=1e-3, fine_tune_start_epoch=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π ResNet18\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # –ó–∞–º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –Ω–∞ 10 –∫–ª–∞—Å—Å–æ–≤ (CIFAR-10)\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞ –±—ç–∫–±–æ–Ω–∞ (–∫—Ä–æ–º–µ fc)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "\n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ ClearML\n",
    "        task.get_logger().report_scalar(\"Loss\", \"train\", loss.item(), self.global_step)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True)\n",
    "\n",
    "        task.get_logger().report_scalar(\"Loss\", \"val\", loss.item(), self.global_step)\n",
    "        task.get_logger().report_scalar(\"Accuracy\", \"val\", acc.item(), self.global_step)\n",
    "\n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ ClearML\n",
    "        if batch_idx == 0:\n",
    "            _, predicted = outputs.max(1)\n",
    "            for i in range(min(len(inputs), config[\"num_images_to_log\"])):\n",
    "                img = transforms.ToPILImage()(inputs[i].cpu())\n",
    "                pred_label = predicted[i].item()\n",
    "                true_label = labels[i].item()\n",
    "                title = f\"True: {classes[true_label]}, Pred: {classes[pred_label]}\"\n",
    "                task.get_logger().report_image(\n",
    "                    title=title,\n",
    "                    series=f\"Epoch {self.current_epoch}\",\n",
    "                    iteration=self.global_step,\n",
    "                    image=img,\n",
    "                )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.model.fc.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –±—ç–∫–±–æ–Ω –ø–æ—Å–ª–µ N-–π —ç–ø–æ—Ö–∏\n",
    "        if self.current_epoch >= self.hparams.fine_tune_start_epoch:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(f\"üîì –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ –≤—Å–µ—Ö —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ {self.current_epoch}-–π —ç–ø–æ—Ö–µ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**üîπ –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∫–æ–¥?**\n",
    "1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é `ResNet18` –∏–∑ `torchvision.models`\n",
    "2. –ú–µ–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –Ω–∞ 10 –∫–ª–∞—Å—Å–æ–≤ (CIFAR-10)\n",
    "3. –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤–µ—Å–∞ –±—ç–∫–±–æ–Ω–∞ (–ø–µ—Ä–≤—ã–µ `fine_tune_start_epoch` —ç–ø–æ—Ö)\n",
    "4. –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç –≤—Å—é –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ `fine_tune_start_epoch`\n",
    "5. –õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ `ClearML`\n",
    "6. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ `val_acc`\n",
    "\n",
    "**üîπ –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Fine-Tuning?**\n",
    "* –í –ø–µ—Ä–≤—ã–µ `fine_tune_start_epoch` —ç–ø–æ—Ö–∏ —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π (fc).\n",
    "* –ó–∞—Ç–µ–º —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ—Ç—Å—è –≤–µ—Å—å `ResNet18` –∏ –æ–±—É—á–∞–µ—Ç—Å—è –≤—Å—è –º–æ–¥–µ–ª—å.\n",
    "* üî• –≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤ `Transfer Learning`, –∫–æ—Ç–æ—Ä—ã–π –¥–µ–ª–∞–µ—Ç `fine-tuning` —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ! üöÄ\n",
    "\n",
    "**üîπ –ß—Ç–æ –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è –≤ ClearML?**\n",
    "\n",
    " ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ (`train_loss`, `val_loss`, `val_acc`)\n",
    " \n",
    " ‚úÖ –û—Ç–ª–∞–¥–æ—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    " \n",
    " ‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    " \n",
    " ‚úÖ –ú–æ–¥–µ–ª—å —Å –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connecting multiple input models with the same name: `resnet18-f37072fd`. This might result in the wrong model being used when executing remotely\n"
     ]
    }
   ],
   "source": [
    "model = FineTuneResNet(\n",
    "    num_classes=config[\"num_classes\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    fine_tune_start_epoch=config[\"fine_tune_start_epoch\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor=\"val_acc\", mode=\"max\", save_top_k=1),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | ResNet           | 11.2 M | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc979de5ce44168834c8afe6f8c014f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å\n",
    "trainer = Trainer(\n",
    "    max_epochs=config[\"num_epochs\"],\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "\n",
    "# ‚úÖ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "–í–∏–¥–Ω–æ –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ \"—É–º–Ω–µ–µ—Ç\" - –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç –æ—à–∏–±–∞—Ç—å—Å—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤.\n",
    "\n",
    "<img src='../images/cif10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> üß∂ –°–≤—è–∑–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ üß©</center>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "–†–∞–∑–±–µ—Ä—ë–º –ø—Ä–∏–º–µ—Ä —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º `PyTorch Lightning` –∏ `ClearML`. –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å `FCN-ResNet50` –∏–∑ `torchvision.models.segmentation` –∏ –æ–±—É—á–∞—Ç—å –µ—ë –Ω–∞ `VOC Segmentation Dataset (Pascal VOC 2012)`.\n",
    "\n",
    "`VOC Segmentation Dataset (Pascal VOC 2012)` - —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ 20 –∫–ª–∞—Å—Å–æ–≤ –∏ 1 —Ñ–æ–Ω–æ–≤—ã–π –∫–ª–∞—Å—Å.\n",
    "\n",
    "–¢–∞–∫ –∂–µ –∏–∑ `torchmetrics` –≤–æ–∑—å–º—ë–º `JaccardIndex` –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ `IoU`.\n",
    "\n",
    "–í –∏—Ç–æ–≥–µ —Å–æ–±—Ä–∞–ª–∏ —Ç–æ–ø–æ–≤—É—é —Å–≤—è–∑–∫—É: **‚ö° Lightning + ClearML + torchmetrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=15137606038b47a68be51eaad6b8dc1d\n",
      "2025-02-20 09:22:17,134 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/a0f504e0e51b42c7bc9d4531c8c2cf60/experiments/15137606038b47a68be51eaad6b8dc1d/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –≤ ClearML\n",
    "task = Task.init(project_name=\"VOC-Segmentation\", \n",
    "                 task_name=\"Fine-Tuning FCN-ResNet50-VOC\",\n",
    "                 task_type=Task.TaskTypes.training\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'dataset': 'VOC2012', 'data_root': './data', 'image_size': (520, 520), 'batch_size': 8, 'num_workers': 4, 'model_name': 'fcn_resnet50', 'pretrained': True, 'num_classes': 21, 'learning_rate': 0.001, 'num_epochs': 10, 'optimizer': 'Adam', 'scheduler': 'ReduceLROnPlateau', 'scheduler_params': {'mode': 'min', 'factor': 0.5, 'patience': 2}, 'image_normalization': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}, 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –≤ ClearML)\n",
    "config = {\n",
    "    # Data parameters\n",
    "    \"dataset\": \"VOC2012\",\n",
    "    \"data_root\": \"./data\",\n",
    "    \"image_size\": (520, 520),\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 4,\n",
    "    \n",
    "    # Model parameters\n",
    "    \"model_name\": \"fcn_resnet50\",\n",
    "    \"pretrained\": True,\n",
    "    \"num_classes\": 21,  # VOC has 20 classes + background\n",
    "    \n",
    "    # Training parameters\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 10,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"scheduler_params\": {\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": 0.5,\n",
    "        \"patience\": 2\n",
    "    },\n",
    "    \n",
    "    # Transforms\n",
    "    \"image_normalization\": {\n",
    "        \"mean\": [0.485, 0.456, 0.406],\n",
    "        \"std\": [0.229, 0.224, 0.225]\n",
    "    },\n",
    "    \n",
    "    # Hardware\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "config = task.connect(config)\n",
    "print(\"Hyperparameters:\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö (Pascal VOC 2012)\n",
    "def get_dataloaders(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=config[\"image_normalization\"][\"mean\"], \n",
    "            std=config[\"image_normalization\"][\"std\"]\n",
    "        ),\n",
    "        transforms.Resize(config['image_size'], antialias=True)\n",
    "    ])\n",
    "\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.Resize(config['image_size'], interpolation=transforms.InterpolationMode.NEAREST)\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.VOCSegmentation(root=config[\"data_root\"], year=\"2012\", image_set=\"train\", \n",
    "                                             download=True, transform=transform, target_transform=target_transform)\n",
    "    val_dataset = datasets.VOCSegmentation(root=config[\"data_root\"], year=\"2012\", image_set=\"val\", \n",
    "                                           download=True, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    # Log dataset information\n",
    "    task.get_logger().report_text(\n",
    "        f\"Train dataset size: {len(train_dataset)} samples\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    task.get_logger().report_text(\n",
    "        f\"Validation dataset size: {len(val_dataset)} samples\", \n",
    "        level=logging.INFO\n",
    "    )\n",
    "\n",
    "    # –ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ VOC \n",
    "    voc_class_names = [\n",
    "        'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', \n",
    "        'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', \n",
    "        'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', \n",
    "        'train', 'tvmonitor']\n",
    "    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤\n",
    "    task.connect({\"class_mapping\": {i: name for i, name in enumerate(voc_class_names)}})\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=config[\"num_workers\"])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=config[\"num_workers\"])\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms: {'image_transforms': ['ToTensor()', 'Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])', 'Resize(size=(520, 520), antialias=True)'], 'target_transforms': ['PILToTensor()', 'Resize(size=(520, 520), interpolation=InterpolationMode.NEAREST)']}\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–ª–æ–≥–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "transforms_config = {\n",
    "    \"image_transforms\": [\n",
    "        \"ToTensor()\",\n",
    "        f\"Normalize(mean={config['image_normalization']['mean']}, std={config['image_normalization']['std']})\",\n",
    "        f\"Resize(size={config['image_size']}, antialias=True)\"\n",
    "    ],\n",
    "    \"target_transforms\": [\n",
    "        \"PILToTensor()\",\n",
    "        f\"Resize(size={config['image_size']}, interpolation=InterpolationMode.NEAREST)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Log configuration to ClearML\n",
    "task.connect_configuration({\"transforms_config\": transforms_config})\n",
    "print(\"Transforms:\", transforms_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ –ú–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º FCN-ResNet50\n",
    "import torchmetrics\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from PIL import Image\n",
    "\n",
    "class SegmentationModel(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.lr = config[\"learning_rate\"]\n",
    "        \n",
    "        # Log model architecture configuration\n",
    "        self.model_config = {\n",
    "            \"backbone\": \"resnet50\",\n",
    "            \"head\": \"FCN\",\n",
    "            \"pretrained\": config[\"pretrained\"],\n",
    "            \"num_classes\": self.num_classes,\n",
    "        }\n",
    "        \n",
    "        # Load pretrained FCN-ResNet50 model\n",
    "        if config[\"pretrained\"]:\n",
    "            self.model = fcn_resnet50(weights=FCN_ResNet50_Weights.DEFAULT)\n",
    "        else:\n",
    "            self.model = fcn_resnet50(weights=None)\n",
    "            \n",
    "        self.model.classifier[4] = nn.Conv2d(512, self.num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.val_iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        \n",
    "        # Class IoU metrics for detailed analysis\n",
    "        self.val_class_iou = torchmetrics.JaccardIndex(\n",
    "            task=\"multiclass\", \n",
    "            num_classes=self.num_classes,\n",
    "            average=None\n",
    "        )\n",
    "        \n",
    "        # ClearML logger\n",
    "        self.clearml_logger = task.get_logger()\n",
    "        \n",
    "        # Log model architecture\n",
    "        self.clearml_logger.report_text(\n",
    "            f\"Model Architecture: {self.model_config}\",\n",
    "            level=logging.INFO\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        masks = masks.squeeze(1).long()  # Convert [B, 1, H, W] to [B, H, W]\n",
    "        \n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, masks)\n",
    "        \n",
    "        # Calculate IoU\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        iou = self.train_iou(preds, masks)\n",
    "        \n",
    "        # Log metrics with ClearML\n",
    "        self.clearml_logger.report_scalar(\"Train/Loss\", \"CE\", loss.item(), self.global_step)\n",
    "        self.clearml_logger.report_scalar(\"Train/IoU\", \"Mean\", iou.item(), self.global_step)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        masks = masks.squeeze(1).long()\n",
    "        \n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, masks)\n",
    "        \n",
    "        # Calculate IoU\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        iou = self.val_iou(preds, masks)\n",
    "        \n",
    "        # Calculate per-class IoU\n",
    "        self.val_class_iou(preds, masks)\n",
    "        \n",
    "        # Log validation images periodically\n",
    "        if batch_idx == 0:\n",
    "            self._log_images(images, masks, preds)\n",
    "            \n",
    "        return {\"val_loss\": loss, \"val_iou\": iou}\n",
    "    \n",
    "    def on_validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_iou = self.val_iou.compute()\n",
    "        \n",
    "        # Get per-class IoU and log it\n",
    "        class_ious = self.val_class_iou.compute()\n",
    "        for i, class_iou in enumerate(class_ious):\n",
    "            self.clearml_logger.report_scalar(\n",
    "                f\"Val/ClassIoU\", \n",
    "                voc_class_names[i], \n",
    "                class_iou.item(), \n",
    "                self.current_epoch\n",
    "            )\n",
    "        \n",
    "        # Reset metrics for next epoch\n",
    "        self.val_iou.reset()\n",
    "        self.val_class_iou.reset()\n",
    "        \n",
    "        # Log with ClearML\n",
    "        self.clearml_logger.report_scalar(\"Val/Loss\", \"CE\", avg_loss.item(), self.current_epoch)\n",
    "        self.clearml_logger.report_scalar(\"Val/IoU\", \"Mean\", avg_iou.item(), self.current_epoch)\n",
    "        \n",
    "        self.log(\"val_loss\", avg_loss, prog_bar=True)\n",
    "        self.log(\"val_iou\", avg_iou, prog_bar=True)\n",
    "    \n",
    "    def _log_images(self, images, masks, predictions, max_imgs=4):\n",
    "        \"\"\"Log images with ClearML for visualization\"\"\"\n",
    "        n_imgs = min(max_imgs, images.shape[0])\n",
    "        \n",
    "        # Create a colormap for segmentation masks\n",
    "        def colorize_mask(mask):\n",
    "            # Simple colormap (could use a more sophisticated one)\n",
    "            cmap = plt.cm.get_cmap('tab20', self.num_classes)\n",
    "            colored = cmap(mask.cpu().numpy())\n",
    "            return Image.fromarray(colored[:, :, :3])\n",
    "        \n",
    "        for idx in range(n_imgs):\n",
    "            # Original image\n",
    "            img = images[idx].cpu().permute(1, 2, 0).numpy()\n",
    "            img = np.clip((img * np.array(self.config[\"image_normalization\"][\"std\"]) + \n",
    "                          np.array(self.config[\"image_normalization\"][\"mean\"])), 0, 1)\n",
    "            img = Image.fromarray(img)\n",
    "            \n",
    "            # Ground truth mask\n",
    "            gt_mask = colorize_mask(masks[idx])\n",
    "            \n",
    "            # Predicted mask\n",
    "            pred_mask = colorize_mask(predictions[idx])\n",
    "            \n",
    "            # Log to ClearML\n",
    "            self.clearml_logger.report_image(title=\"Validation Images\", series=\"Input Image\", iteration=self.global_step, image=img)\n",
    "            self.clearml_logger.report_image(title=\"Validation Images\", series=\"GT Mask\", iteration=self.global_step, image=gt_mask)\n",
    "            self.clearml_logger.report_image(title=\"Validation Images\", series=\"Predicted Mask\", iteration=self.global_step, image=pred_mask)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        \n",
    "        if self.config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, \n",
    "                mode=self.config[\"scheduler_params\"][\"mode\"],\n",
    "                factor=self.config[\"scheduler_params\"][\"factor\"], \n",
    "                patience=self.config[\"scheduler_params\"][\"patience\"],\n",
    "                verbose=True\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"interval\": \"epoch\",\n",
    "                    \"frequency\": 1\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**üîπ –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?**\n",
    "1. `FCN-ResNet50` –∏–∑ `torchvision`:\n",
    "\n",
    "* –ú—ã –±–µ—Ä–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π `FCN-ResNet50`.\n",
    "* –¢–∞–∫ –∂–µ –º–æ–∂–µ–º –Ω–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.\n",
    "* –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –ø–µ—Ä–µ–¥–µ–ª—ã–≤–∞–µ–º –¥–ª—è `num_classes` (21).\n",
    "  \n",
    "2. `JaccardIndex` –∏–∑ `torchmetrics`:\n",
    "\n",
    "* –ë–µ—Ä—ë–º `JaccardIndex` –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ `IoU` - –º–µ—Ç—Ä–∏–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "* –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫—É –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ, –ø–æ—Ç–æ–º —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–∞–Ω—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ–π —ç–ø–æ—Ö–µ.\n",
    "  \n",
    "3. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ `ClearML`:\n",
    "\n",
    "* –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∫–∏\n",
    "* `train_loss`, `val_loss`\n",
    "* `train_iou`, `val_iou`\n",
    "* –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –≤—Ö–æ–¥–Ω–æ–µ, –º–∞—Å–∫–∞ Ground Truth, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–∞—Å–∫–∞.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported key of type '<class 'int'>' found when connecting dictionary. It will be converted to str\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1464 samples\n",
      "Validation dataset size: 1449 samples\n",
      "2025-02-20 09:26:48,384 - clearml.model - INFO - Selected model id: f35a6f20500b43eebca2572247ae0168\n",
      "Model Architecture: {'backbone': 'resnet50', 'head': 'FCN', 'pretrained': True, 'num_classes': 21}\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏\n",
    "train_loader, val_loader = get_dataloaders(config[\"batch_size\"])\n",
    "model = SegmentationModel(config)\n",
    "\n",
    "# ‚úÖ Callbacks (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ LR)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='val_iou', \n",
    "                    filename='fcn-resnet50-voc-{epoch:02d}-{val_iou:.3f}',\n",
    "                    save_top_k=2,\n",
    "                    mode='max',\n",
    "                    save_last=True\n",
    "                   ),\n",
    "    LearningRateMonitor(logging_interval=\"step\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sinarian/venvs/sinara/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: PossibleUserWarning:\n",
      "\n",
      "GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å\n",
    "seed_everything(42)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config[\"num_epochs\"],\n",
    "    accelerator=\"cpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    #devices=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ –ù–µ –∑–∞–±—ã–≤–∞–µ–º –∑–∞–∫—Ä—ã—Ç—å —Ç–∞—Å–∫\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—ç–º–ø–ª—ã (–≤–∫–ª–∞–¥–∫–∞ `debug`)\n",
    "\n",
    "<img src='../images/seg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center id=\"p6\"> üß∏ –í—ã–≤–æ–¥—ã –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏—è ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "–í —É—Ä–æ–∫–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ —Å–ø–æ—Å–æ–±—ã –∫–∞–∫ —Å–≤—è–∑–∫–∞ `Lightning + ClearML` –ø–æ–º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –∏ –æ—Ç–ª–∞–¥–∫—É –≤ CV-–∑–∞–¥–∞—á–∞—Ö:    \n",
    "* –†–∞–∑–æ–±—Ä–∞–ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "* –§–∞–π–Ω—Ç—é–Ω–∏–Ω–≥ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ –±—ç–∫–±–æ–Ω–∞\n",
    "* –ü–æ–Ω—è–ª–∏ –∫–∞–∫ –ª—É—á—à–µ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –∫–æ–¥ –¥–ª—è –∑–∞–¥–∞—á–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "* –í –î–ó –ø–æ–ø—Ä–∞–∫—Ç–∏–∫—É–µ–º—Å—è –æ–±—É—á–∞—Ç—å GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
